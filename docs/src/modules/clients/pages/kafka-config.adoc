Configuration parameters for the Kafka Clients. It accepts any configuration property that can be set in the Kafka Clients:

* `org.apache.kafka.clients.consumer.KafkaConsumer`
* `org.apache.kafka.clients.producer.KafkaProducer`

For more information on the available configuration properties, see *Kafka Configuration Reference*:

* Producer: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html[window=_blank]
* Consumer: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html[window=_blank]
* Schema Registry: https://docs.confluent.io/platform/current/schema-registry/connect.html#configuration-options[window=_blank]

NOTE: If the project has been generated using the xref:archetype:index.adoc[Karate Tools Archetype] the archetype would have prompted for the creation of the configuration files.

This client can be configured for multi-environment execution with a *config file per environment*:

[source,plaintext,subs="+attributes"]
----
\---src
    \---test
        \---resources
            \---config
                \---kafka
                    kafka-config-local.yml
                    ...
                    kafka-config-pre.yml
----

=== KafkaClient config - Kafka Cluster and Schema Registry Servers

* *`bootstrap.servers`*: Comma-separated list of host/port pairs to use for establishing the initial connection to the Kafka cluster. For example:
** `localhost:39092`

* *`schema.registry.url`*: Comma-separated list of URLs (protocol://host:port) for Schema Registry instances that can be used to register or look up schemas. For example:
** `http://localhost:38082`

=== KafkaClient config - Producer

IMPORTANT: *Kafka Producer* specific properties - must start with `producer.` so they are only applied to a Producer client.

* *`producer.client.id`*: An id string to pass to the server when making requests. This value is used to identify the client application.

* *`producer.auto.register.schemas`*: Specify if the Serializer should attempt to register the Schema with Schema Registry. *To be used with AVRO Serializer.*
** *`true`*: Automatically register the Schema with Schema Registry. *This is the value to be set for LOCAL environment*
** *`false`*: Do not register the Schema with Schema Registry. *This is the valued to be set for REMOTE environments with other mechanisms to register the schema.*

* *`producer.key.subject.name.strategy`*: Determines how to construct the subject name under which the key schema is registered with Schema Registry. *To be used with AVRO Serializer if a custom strategy is needed.*

* *`producer.value.subject.name.strategy`*: Determines how to construct the subject name under which the value schema is registered with Schema Registry. *To be used with AVRO Serializer if a custom strategy is needed.*

* *`producer.key.serializer`*: Serializer class for key that implements the `org.apache.kafka.common.serialization.Serializer` interface. For example:
** `org.apache.kafka.common.serialization.LongSerializer`
** `org.apache.kafka.common.serialization.StringSerializer`

* *`producer.value.serializer`*: Serializer class for value that implements the `org.apache.kafka.common.serialization.Serializer` interface. For example:
** `org.apache.kafka.common.serialization.StringSerializer`
** `io.confluent.kafka.serializers.KafkaAvroSerializer`

=== KafkaClient config - Consumer

IMPORTANT: *Kafka Consumer* specific properties - must start with `consumer.` so they are only applied to a Consumer client.

* *`consumer.group.id`*: A unique string that identifies the consumer group this consumer belongs to.
** This value will be used as default group for the consumer
** If not provided, it will use the default value `karate-kafka-default-consumer-group`
** It can be overwritten in specific calls like `consume(final String topic, final String group)`

* *`consumer.key.deserializer`*: Deserializer class for key that implements the `org.apache.kafka.common.serialization.Deserializer` interface
** Kafka Consumer Serializer class for key Example Values:
*** `org.apache.kafka.common.serialization.LongDeserializer`
*** `org.apache.kafka.common.serialization.StringDeserializer`

* *`consumer.value.deserializer`*: Deserializer class for value that implements the `org.apache.kafka.common.serialization.Deserializer` interface
** Kafka Consumer Deserializer class for value Example Values:
*** `org.apache.kafka.common.serialization.StringDeserializer`
*** `io.confluent.kafka.serializers.KafkaAvroDeserializer`

* *`consumer.auto.offset.reset`*: What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted).
** Valid values are ( *Anything else* will throw exception to the consumer):
*** `earliest`: Automatically reset the offset to the earliest offset
*** `latest`: Automatically reset the offset to the latest offset
*** `none`: Throw exception to the consumer if no previous offset is found for the consumer's group
** This value will be used as default value for the consumer.
** If not provided, it will use the default value `earliest`
** It can be overwritten in specific calls like `consume(final String topic, final String group, final String offset)`

=== KafkaClient config - Authentication

To access *Secure Kafka clusters*, the following *authentication properties* must be set:

* *`security.protocol`*: Protocol used to communicate with brokers. Valid values are: SASL_PLAINTEXT, SASL_SSL

* *`sasl.mechanism`*: Mechanism used for client connections. Valid values are: PLAIN, SCRAM-SHA-512

* *`sasl.jaas.config`*: JAAS login context parameters for SASL connections.
** For example:
*** `org.apache.kafka.common.security.plain.PlainLoginModule required username='...' password='...';`
*** `org.apache.kafka.common.security.scram.ScramLoginModule required username='...' password='...';`
